{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本自动求导（Automatic Differentiation AD）\n",
    "\n",
    "自动求导有两种形式，其分别为前向模式（Forward Mode）和反向模式（Reverse Mode）\n",
    "\n",
    "* Forward Mode依靠**二元数**，在前向运算过程中同时计算值和导数值\n",
    "* Reverse Mode分为两个步骤，第一步为前向运算，第二步为反向传播\n",
    "\n",
    "参考论文：[Automatic Differentiation in Machine Learning: a Survey](https://arxiv.org/pdf/1502.05767.pdf)\n",
    "\n",
    "参考实现：[autograd-by-borgwang](https://github.com/borgwang/toys/blob/master/ml-autograd/autograd.ipynb)\n",
    "\n",
    "参考解析：\n",
    "\n",
    "1. [神经网络自动求导的设计与实现](https://zhuanlan.zhihu.com/p/82582926)\n",
    "\n",
    "2. [tensorflow的函数自动求导是如何实现的](https://www.zhihu.com/question/54554389)\n",
    "\n",
    "在这里，由于是对神经网络的分析，因此主要讨论自动求导的**Reverse Mode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AD实现要点\n",
    "\n",
    "对于AD，其在运算过程中需要记录如下数据\n",
    "\n",
    "* 当前运算的运算结果\n",
    "* 当前运算的梯度计算方法\n",
    "* 当前运算直接依赖的前置运算\n",
    "\n",
    "## AD过程：\n",
    "\n",
    "假设已完成前向计算，并保留了上述需要的数据\n",
    "\n",
    "* 从整个运算的结果出发，记录当前运算的梯度，并将梯度传递给当前运算直接依赖的前置运算\n",
    "* 递归上述步骤，直到所有计算结束\n",
    "\n",
    "## 基于AD的神经网络优化流程\n",
    "* **运算定义**：若要实现AD，每一个运算不仅要计算结果，还需要对运算的前置依赖关系以及梯度计算方法进行记录\n",
    "* **反向传播**: 在完成正向计算后，需要进行反向传播\n",
    "* **参数更新**：在完成反向传播后，依靠梯度计算结果更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_tensor(obj):\n",
    "    if not isinstance(obj, MyTensor):\n",
    "        obj = MyTensor(obj)\n",
    "    return obj\n",
    "\n",
    "\n",
    "class MyTensor(object):\n",
    "\n",
    "    def __init__(self, values, requires_grad=False, dependency=None):\n",
    "        self._values = np.array(values)\n",
    "        self._shape = self._values.shape\n",
    "\n",
    "        self.grad = None\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        if self.requires_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "        if dependency is None:\n",
    "            self.dependency = list()\n",
    "        else:\n",
    "            self.dependency = dependency\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.grad = np.zeros(self._shape)\n",
    "    \n",
    "    @property\n",
    "    def values(self):\n",
    "        return self._values\n",
    "    \n",
    "    @ property\n",
    "    def shape(self):\n",
    "        return self._shape\n",
    "    \n",
    "    @values.setter\n",
    "    def values(self, values):\n",
    "        self._values = np.array(values)\n",
    "        self.grad = None\n",
    "\n",
    "    def backward(self, grad=None):\n",
    "        \"\"\"\n",
    "        反向传播核心代码，累计当前tensor的梯度，同时反向传播梯度\n",
    "        \"\"\"\n",
    "        assert self.requires_grad, \"Call backward() on a non-requires-grad tensor\"\n",
    "\n",
    "        if grad is None:\n",
    "            grad = 1.0\n",
    "        grad = np.array(grad)\n",
    "\n",
    "        # 梯度叠加\n",
    "        self.grad += grad\n",
    "\n",
    "        # 反向传播梯度到直接依赖的运算\n",
    "        for dep in self.dependency:\n",
    "            # 注意这里是直接将梯度传播到依赖的运算，而不是传播梯度叠加的结果\n",
    "            # 其原因在于这里使用的是深度优先遍历\n",
    "            # 由于在反向传播梯度后，会进行直接依赖的运算反向传播，因此是深度优先\n",
    "            # 这么一来在遍历完成后，所有经过当前运算传递到依赖运算的路径均会被搜索到\n",
    "            \n",
    "            # 比如\n",
    "            # 假设当前节点前有两个后继节点\n",
    "            # 那么反向传播流程为\n",
    "\n",
    "            # 梯度清零\n",
    "            # 后继节点一反向传播，当前节点梯度+grad_1\n",
    "            # 当前节点继续反向传播，前置节点梯度+grad_1\n",
    "            # 另一条路径：后继节点二反向传播，当前节点梯度+grad_2\n",
    "            # 当前节点继续反向传播，前置节点梯度+grad_2\n",
    "\n",
    "            # 若在上述过程中每次传递的是累计梯度，则会导致前置节点梯度+grad_1+(grad_1+grad_2)\n",
    "            # 即在第二条路径中对第一条路径的梯度又叠加了一次\n",
    "            grad_cal_method_for_dep = dep[\"grad_func\"](grad)\n",
    "            # 迭代调用直接依赖的运算的反向传播\n",
    "            dep[\"tensor\"].backward(grad_cal_method_for_dep)\n",
    "    \n",
    "    # 运算重载，对于反向传播，其不仅要计算值，还需要计算梯度\n",
    "    # 重载运算\n",
    "\n",
    "    # 二元运算：\n",
    "    # 矩阵乘法：左乘、右乘、原位乘\n",
    "    # 加法：左加、右加、原位加\n",
    "    # 减法：左减、右减、原位减\n",
    "    # 按位乘法：左乘、右乘、原位乘\n",
    "\n",
    "    # 一元运算：\n",
    "    # 取反\n",
    "    # sigmoid\n",
    "    # ReLU\n",
    "    # LeakyReLU\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        右加法重载\n",
    "        \n",
    "        return: self + other\n",
    "        \"\"\"\n",
    "        return self._add(self, as_tensor(other))\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        \"\"\"\n",
    "        左加法重载\n",
    "\n",
    "        return: other + self\n",
    "        \"\"\"\n",
    "        return self._add(as_tensor(other), self)\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        \"\"\"\n",
    "        原位加法重载\n",
    "\n",
    "        self += other\n",
    "\n",
    "        原位加法不会生成新的节点，因此不会记录梯度\n",
    "        \"\"\"\n",
    "        self.values = self.values + as_tensor(other).values\n",
    "        return self\n",
    "\n",
    "    def _add(self, tensor1, tensor2):\n",
    "        \"\"\"\n",
    "        运算为：c = a + b\n",
    "\n",
    "        Dc/Da = 1\n",
    "        Dc/Db = 1\n",
    "\n",
    "        在本运算中，强制要求 a.shape == b.shape == c.shape\n",
    "        \"\"\"\n",
    "        if tensor1.shape != tensor2.shape:\n",
    "            raise RuntimeError(\"Add expects each tensor to be equal size, but got {} at entry 0 and {} at entry 1\".format(tensor1.shape, tensor2.shape))\n",
    "        _result = tensor1 + tensor2\n",
    "\n",
    "        def grad_func_tensor1(grad):\n",
    "            return grad\n",
    "\n",
    "        def grad_func_tensor2(grad):\n",
    "            return grad\n",
    "        \n",
    "        return self.build_binary_ops_result_tensor(tensor1, tensor2, grad_func_tensor1, grad_func_tensor2, _result)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \"\"\"\n",
    "        右减法重载\n",
    "        \n",
    "        return: self - other\n",
    "        \"\"\"\n",
    "        return self._sub(self, as_tensor(other))\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        \"\"\"\n",
    "        左减法重载\n",
    "\n",
    "        return：other - self\n",
    "        \"\"\"\n",
    "        return self._sub(as_tensor(other), self)\n",
    "    \n",
    "    def __isub__(self, other):\n",
    "        \"\"\"\n",
    "        原位减法\n",
    "        self -= other\n",
    "\n",
    "        原位减法不会生成新的节点，因此不会记录梯度\n",
    "        \"\"\"\n",
    "        self.values = self.values - as_tensor(other).values\n",
    "        return self\n",
    "\n",
    "    def _sub(self, tensor1, tensor2):\n",
    "        \"\"\"\n",
    "        运算为：c = a - b\n",
    "\n",
    "        Dc/Da = 1\n",
    "        Dc/Db = -1\n",
    "        在本运算中，强制要求 a.shape == b.shape == c.shape\n",
    "        \"\"\"\n",
    "        if tensor1.shape != tensor2.shape:\n",
    "            raise RuntimeError(\"Add expects each tensor to be equal size, but got {} at entry 0 and {} at entry 1\".format(tensor1.shape, tensor2.shape))\n",
    "        _result = tensor1 - tensor2\n",
    "\n",
    "        def grad_func_tensor1(grad):\n",
    "            return grad\n",
    "\n",
    "        def grad_func_tensor2(grad):\n",
    "            return -grad\n",
    "        \n",
    "        return self.build_binary_ops_result_tensor(tensor1, tensor2, grad_func_tensor1, grad_func_tensor2, _result)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        矩阵右乘\n",
    "\n",
    "        return self @ other\n",
    "        \"\"\"\n",
    "        return self._matmul(self, as_tensor(other))\n",
    "    \n",
    "    def __rmatmul__(self, other):\n",
    "        \"\"\"\n",
    "        矩阵左乘\n",
    "        \n",
    "        return other @ self\n",
    "        \"\"\"\n",
    "        return self._matmul(as_tensor(other), self)\n",
    "    \n",
    "    def __imatmul__(self, other):\n",
    "        \"\"\"\n",
    "        原位乘法\n",
    "        self @= other\n",
    "        \"\"\"\n",
    "        self.values = self.values @ as_tensor(other).values\n",
    "        return self\n",
    "\n",
    "    def _matmul(self, tensor1, tensor2):\n",
    "        \"\"\"\n",
    "        运算为: c = a @ b\n",
    "\n",
    "        Dc/Da = grad @ b.T\n",
    "        Dc/Da = a.T @ grad\n",
    "        \"\"\"\n",
    "        if tensor1.shape != tensor2.shape:\n",
    "            raise RuntimeError(\"RuntimeError: size mismatch, m1: {}, m2: {}\".format(tensor1.shape, tensor2.shape))\n",
    "\n",
    "        _result = tensor1 @ tensor2\n",
    "        def grad_func_tensor1(grad):\n",
    "            return grad @ tensor2.values.T\n",
    "        \n",
    "        def grad_func_tensor2(grad):\n",
    "            return tensor1.values.T @ grad\n",
    "\n",
    "        return self.build_binary_ops_result_tensor(tensor1, tensor2, grad_func_tensor1, grad_func_tensor2, _result)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        取反运算\n",
    "        return -self\n",
    "        \"\"\"\n",
    "        return self._neg(self)\n",
    "    \n",
    "    def _neg(self, tensor):\n",
    "        \"\"\"\n",
    "        运算为: c = -a\n",
    "\n",
    "        Dc/Da = -1\n",
    "        \"\"\"\n",
    "        _result = -tensor\n",
    "\n",
    "        def grad_func(grad):\n",
    "            return -grad\n",
    "        \n",
    "        return self.build_unary_ops_result_tensor(tensor, grad_func, _result)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_binary_ops_result_tensor(tensor1, tensor2, grad_func_tensor1, grad_func_tensor2, values):\n",
    "        \"\"\"\n",
    "        建立二元运算结果的tensor\n",
    "        \"\"\"\n",
    "        requires_grad = tensor1.requires_grad or tensor2.required_grad\n",
    "        \n",
    "        dependency = list()\n",
    "        if tensor1.requires_grad:\n",
    "            dependency.append({\"tensor\": tensor1, \"grad_func\": grad_func_tensor1})\n",
    "        if tensor2.requires_grad:\n",
    "            dependency.append({\"tensor\": tensor2, \"grad_func\": grad_func_tensor2})\n",
    "\n",
    "        return MyTensor(values, requires_grad, dependency)\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_unary_ops_result_tensor(tensor, grad_func, values):\n",
    "        \"\"\"\n",
    "        建立一元运算结果的tensor\n",
    "        \"\"\"\n",
    "\n",
    "        requires_grad = tensor.requires_grad\n",
    "        dependency = list()\n",
    "\n",
    "        if tensor.requires_grad:\n",
    "            dependency.append({\"tensor\": tensor, \"grad_func\": grad_func})\n",
    "        \n",
    "        return MyTensor(values, requires_grad, dependency)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2ae398f54937c2e9a792e0245dde3e7f00c22003a4409849c6155f07537f255"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('DIP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
