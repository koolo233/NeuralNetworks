{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 径向基函数网络/RBF网络\n",
    "\n",
    "RBF网络时具有隐藏层的前向网络，隐藏层以RBF作为激活函数。不同于一般的BP网络，RBF网络具有局部逼近的特性。并且Poggio和Girosi已经证明RBF网络时连续函数的最佳逼近。\n",
    "\n",
    "BP网络和RBF网络的区别见下图\n",
    "\n",
    "![BP网络和RBF网络对比](https://raw.githubusercontent.com/koolo233/NeuralNetworks/main/images/bp_rbf.jpg \"segment\")\n",
    "\n",
    "## RBF网络的正向计算\n",
    "\n",
    "### 定义RBF函数\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\phi(\\bm{x}, \\bm{c}) = \\phi(||\\bm{x}-\\bm{c}||)\n",
    "\\end{equation}\n",
    "$$\n",
    "上式中$c$为中心坐标，$\\phi$为高斯核函数\n",
    "\n",
    "* 以高斯核函数和欧氏距离定义的RBF函数\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\varphi(\\bm{x}) = exp(-\\frac{1}{2}\\sum_{i=1}^{k}\\frac{(x_i-c_i)^2}{\\sigma_i^2})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "上式中，输入维度为$k$，$c_i$和$\\sigma_i$分别为第$i$个维度的中心坐标和标准化参数\n",
    "\n",
    "若所有维度均采用相同的标准化参数，则上式可简化为\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\varphi(\\bm{x}) = exp(-\\frac{1}{2\\sigma^2}||\\bm{x}-\\bm{c}||^2)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "定义\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\beta = \\frac{1}{2\\sigma^2}, \\  \\beta \\geq0\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "有\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\varphi(\\bm{x}) = exp(-\\beta||\\bm{x}-\\bm{c}||^2)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "### 从输入层到隐藏层\n",
    "\n",
    "假设隐藏层有n个神经元，则对于每一个神经元有\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\varphi_i = exp(-(\\bm{x}-\\bm{c_i})^Tdiag(\\bm{x}-\\bm{c_i})\\bm{\\beta_i}), \\ i = 1, 2, 3, \\cdots, n\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### 从隐藏层到输出层\n",
    "假设输出层有o个输出单元，则对于输出层有\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\bm{y} \n",
    "        &= \\bm{W}\\bm{\\varphi} \\\\\n",
    "        &= \n",
    "        \\left[\n",
    "            \\begin{array}{cc}\n",
    "                \\bm{w_1} \\\\\n",
    "                \\bm{w_2} \\\\\n",
    "                \\vdots \\\\\n",
    "                \\bm{w_o} \\\\\n",
    "            \\end{array}\n",
    "        \\right]\n",
    "        \\left[\n",
    "            \\begin{array}{cc}\n",
    "                \\varphi_1 \\\\\n",
    "                \\varphi_2 \\\\\n",
    "                \\vdots \\\\\n",
    "                \\varphi_n\n",
    "            \\end{array}\n",
    "        \\right]\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## RBF网络的学习算法\n",
    "\n",
    "RBF网络待学习参数如下：\n",
    "1. 高斯RBF函数的中心坐标$\\bm{c}$和参数$\\beta$\n",
    "2. 隐藏层到输出层的权重$\\bm{W}$\n",
    "\n",
    "### 自组织选取中心学习算法\n",
    "1. 无监督学习过程：求解高斯RBF函数相关参数\n",
    "2. 有监督学习过程：求解隐藏层到输出层的权重\n",
    "\n",
    "对于第一步，可以使用聚类相关方法来进行。即对于n个RBF单元，通过对输入样本聚类为n类来获得相应的中心坐标以及散度参数。\n",
    "\n",
    "\n",
    "在完成第一步后，第二步为确定隐藏层到输出层的权重。这一步为线性映射，因此可以使用各种线性优化算法来求得最优解，例如最小二乘法等。\n",
    "\n",
    "### 监督学习\n",
    "对于RBF网络，其可以使用类似于一般BP算法中的梯度下降法对网络参数进行求解。在本文件中主要是实现这一优化方法。\n",
    "\n",
    "## RBF网络的监督学习\n",
    "\n",
    "* 定义损失\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L(\\bm{c}, \\bm{\\beta}, \\bm{W})} = \\frac{1}{M}\\sum_{i=1}^M||\\bm{\\bar{y}}_i - \\bm{y}_i||^2_2\n",
    "\\end{equation}\n",
    "$$\n",
    "其中$\\bm{\\bar{y}}_i$为第$i$个样本的输出，$\\bm{y}_i$为第$i$个样本的标签，$M$为训练集的样本总数\n",
    "\n",
    "* 定义优化函数\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\min\\limits_{\\bm{c}, \\bm{\\beta}, \\bm{W}}{\\mathcal{L}(\\bm{c}, \\bm{\\beta}, \\bm{W})}=\\frac{1}{M}\\sum_{i=1}^M||\\bm{\\bar{y}}_i - \\bm{y}_i||^2_2\n",
    "\\end{equation}\n",
    "$$\n",
    "* 误差反向传播(单样本)\n",
    "    * $\\bm{W}$\n",
    "    $$\n",
    "    \\begin{equation}\n",
    "        \\begin{split}\n",
    "            \\frac{\\partial \\mathcal{L}}{\\partial \\bm{W}} \n",
    "            &= \\frac{\\partial \\mathcal{L}}{\\partial \\bm{\\bar{y}}} \\frac{\\partial \\bm{\\bar{y}}}{\\partial \\bm{W}} \\\\\n",
    "            &= 2(\\bm{\\bar{y}}-\\bm{y})\\bm{\\varphi}^T\n",
    "        \\end{split}\n",
    "    \\end{equation}\n",
    "    $$\n",
    "    * $\\bm{\\varphi}$\n",
    "    $$\n",
    "    \\begin{equation}\n",
    "        \\begin{split}\n",
    "            \\frac{\\partial \\mathcal{L}}{\\partial \\bm{\\varphi}}\n",
    "            &= \\frac{\\partial \\mathcal{L}}{\\partial \\bm{\\bar{y}}} \\frac{\\partial \\bm{\\bar{y}}}{\\partial \\bm{\\varphi}} \\\\ \n",
    "            &= 2(\\bm{\\bar{y}}-\\bm{y})^T \\bm{W}\n",
    "        \\end{split}\n",
    "    \\end{equation}\n",
    "    $$\n",
    "    * $\\bm{c}$\n",
    "    对于每一个隐藏单元的中心参数$\\bm{c_i}$有\n",
    "    $$\n",
    "    \\begin{equation}\n",
    "        \\begin{split}\n",
    "            \\frac{\\partial \\mathcal{L}}{\\partial \\bm{c_i}}\n",
    "            &= \\frac{\\partial \\mathcal{L}}{\\partial \\bm{\\varphi_i}} \\frac{\\partial \\bm{\\varphi_i}}{\\partial \\bm{c_i}} \\\\ \n",
    "            &= (\\sum_{j=1}^o2(\\bar{y}_j-y_j)\\bm{W}_{ji})(\\varphi_i)(2\\bm{\\beta_i}^Tdiag(\\bm{x}-\\bm{c_i }))\n",
    "        \\end{split}, i = 1, 2, \\cdots, n\n",
    "    \\end{equation}\n",
    "    $$\n",
    "    * $\\bm{\\beta}$\n",
    "    对于每一个隐藏单元的参数$\\bm{\\beta_i}$有\n",
    "    $$\n",
    "    \\begin{equation}\n",
    "        \\begin{split}\n",
    "            \\frac{\\partial \\mathcal{L}}{\\partial \\bm{\\beta_i}}\n",
    "            &= \\frac{\\partial \\mathcal{L}}{\\partial \\bm{\\varphi_i}} \\frac{\\partial \\bm{\\varphi_i}}{\\partial \\bm{\\beta_i}} \\\\\n",
    "            &= (\\sum_{j=1}^o2(\\bar{y}_j-y_j)\\bm{W}_{ji}) (\\varphi_i)( -(\\bm{x}-\\bm{c_i})^Tdiag(\\bm{x}-\\bm{c_i}))\n",
    "        \\end{split}, i = 1, 2, \\cdots, n\n",
    "    \\end{equation}\n",
    "    $$\n",
    "\n",
    "* 更新参数\n",
    "    * $\\bm{W}$\n",
    "    $$\n",
    "    \\begin{equation}\n",
    "        \\bm{W}_{t+1} = \\bm{W}_t - \\eta\\frac{\\partial \\mathcal{L}}{\\partial \\bm{W}}\n",
    "    \\end{equation}\n",
    "    $$\n",
    "    * $\\bm{c}$\n",
    "    对于每一个隐藏单元的中心参数$\\bm{c_i}$有\n",
    "    $$\n",
    "    \\begin{equation}\n",
    "        \\bm{c_i}_{t+1} = {c_{i}}_t - \\eta (\\frac{\\partial \\mathcal{L}}{\\partial \\bm{c_i}})^T\n",
    "    \\end{equation}\n",
    "    $$\n",
    "    * $\\bm{\\beta}$\n",
    "    对于每一个隐藏单元的参数$\\bm{\\beta_i}$有\n",
    "    $$\n",
    "    \\begin{equation}\n",
    "        \\bm{\\beta_i}_{t+1} = {\\beta_{i}}_t - \\eta (\\frac{\\partial \\mathcal{L}}{\\partial \\bm{\\beta_i}})^T\n",
    "    \\end{equation}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "import os\n",
    "import random\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# seed\n",
    "random.seed(1024)\n",
    "np.random.seed(1024)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFunction(object):\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, lr) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # 模型权重\n",
    "        random.seed(1024)\n",
    "        np.random.seed(1024)\n",
    "        \n",
    "        # center parmas\n",
    "        # hidden_dims, input_dims\n",
    "        self.center_params = np.random.randn(hidden_dims, input_dims)\n",
    "        # normalize params\n",
    "        # hidden_dims, input_dims\n",
    "        self.norm_params = np.random.randn(hidden_dims, input_dims)\n",
    "        # output weights\n",
    "        self.output_weights = np.random.randn(output_dims, hidden_dims+1)\n",
    "\n",
    "        # 中间结果\n",
    "        self.input_data = None\n",
    "        self.hidden_value = None\n",
    "        \n",
    "        # 梯度结果\n",
    "        self.grad_center_params = None\n",
    "        self.grad_norm_params = None\n",
    "        self.grad_output_weights = None\n",
    "\n",
    "        # 学习率\n",
    "        self.lr = lr\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c138c3a39b15f82bd4a9598693589d2dc2de979d592ed7357973539f0f36bd2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('flyai_pytorch1_5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
