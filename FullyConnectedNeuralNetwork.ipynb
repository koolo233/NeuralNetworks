{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP网络/全连接神经网络\n",
    "\n",
    "BP网络为前向映射网络，网络由输入层、隐藏层以及输出层组成。\n",
    "\n",
    "## 基本单元\n",
    "\n",
    "BP网络的基本单元为含有激活函数的单个神经元\n",
    "\n",
    "$$o = f(x) = f(\\omega \\cdot \\textbf{x} + b) \\\\ \\omega = [\\omega_1, \\omega_2, \\cdots, \\omega_n]^T \\\\ x = [x_1, x_2, \\cdots, x_n]^T$$\n",
    "\n",
    "其中，$o$为输出，$\\omega$为权重向量， $\\textbf{x}$ 为输入向量，$b$ 为偏执, $f$为激活函数\n",
    "\n",
    "根据不同的任务类型，可以选用不同的激活函数\n",
    "\n",
    "## 基本架构\n",
    "\n",
    "通过多个基本单元组成一层，然后通过多层组成整个BP网络\n",
    "\n",
    "## BP网络的学习算法\n",
    "\n",
    "BP网络学习算法由两部分组成\n",
    "* 前向计算\n",
    "* 误差反向传播\n",
    "\n",
    "其中，前向传播过程是网络的应用过程；误差反向传播过程是BP网络权值学习和训练过程。\n",
    "\n",
    "**核心思想**：将输出误差以**某种形式**从输出层传播到隐藏层最后到输入层。因此，对于BP网络学习算法，最为重要的就是决定误差传播方式\n",
    "\n",
    "### 符号运算实现误差反向传播\n",
    "\n",
    "符号运算实现的误差反向传播方式要求首先对梯度进行符号运算并表示成已知量的运算，即**显式**的对每一个神经元的梯度运算进行推导。然后通过带入相应的量求得梯度，实现误差的传播和参数更新。\n",
    "\n",
    "### 自动微分实现误差反向传播\n",
    "\n",
    "自动微分实现的误差反向传播本质上是链式法则以及递归思想的综合。其不要求给出**显式**的每一个神经元的梯度计算符号表达式，而是根据链式法则以及递归，以深度优先搜索的方式对整个计算图中的待优化参数进行梯度计算。\n",
    "\n",
    "自动微分实现误差反向传播要求事先给出每一个运算的微分计算方式，即每一个运算不仅要给出其运算结果还需要给出其对应的微分计算方式。**对于不可导的位置，需要手动给出对应的微分结果**，例如在ReLU激活函数中，0位置不可导，因此需要手动赋予该处的导数为0.\n",
    "\n",
    "## 三层BP网络误差传递的符号推导\n",
    "\n",
    "### 定义\n",
    "定义一个三层BP网络，即输入层、一层隐藏层以及输出层\n",
    "\n",
    "参数定义如下：\n",
    "\n",
    "* 输入向量\n",
    "$$\\begin{equation} \\textbf{x} = (x_1, x_2, \\cdots, x_n)^T, \\textbf{x}\\in\\mathcal{R}^n \\end{equation}$$\n",
    "* 输入层到隐藏层权重\n",
    "$$\\begin{equation} \\textbf{V}=\\left[\\begin{array}{cc} V_{11}&V_{12}& \\cdots&V_{1n} \\\\ \\vdots&\\vdots&\\ddots&\\vdots \\\\ V_{m1}&V_{m2}&\\cdots&V_{mn} \\end{array}\\right], \\textbf{V}\\in\\mathcal{R}^{m\\times n} \\end{equation}$$\n",
    "* 隐藏层激活函数\n",
    "$$\\begin{equation} f_1(\\textbf{V}\\textbf{x}) \\end{equation}$$\n",
    "* 隐藏层输出\n",
    "$$\\begin{equation} \\textbf{h} = (h_1, h_2, \\cdots, h_m)^T, \\textbf{h}\\in\\mathcal{R}^m \\end{equation}$$\n",
    "* 隐藏层到输出权重\n",
    "$$\\begin{equation} \\textbf{W} = \\left[\\begin{array}{cc} W_{11}&W_{12}& \\cdots&W_{1m} \\\\ \\vdots&\\vdots&\\ddots&\\vdots \\\\ W_{k1}&W_{k2}&\\cdots&W_{km} \\end{array}\\right], \\textbf{W}\\in\\mathcal{R}^{k\\times m} \\end{equation}$$\n",
    "* 输出层激活函数\n",
    "$$\\begin{equation} f_2(\\textbf{\\textbf{W}\\textbf{h}}) \\end{equation}$$\n",
    "* 输出层输出\n",
    "$$\\begin{equation} \\textbf{o} = (o_1, o_2, \\cdots, o_k)^T, \\textbf{o}\\in\\mathcal{R}^k \\end{equation}$$\n",
    "* 期望输出\n",
    "$$\\begin{equation} \\textbf{y} = (y_1, y_2, \\cdots, y_k)^T, \\textbf{o}\\in\\mathcal{R}^k \\end{equation}$$\n",
    "\n",
    "### 正向运算过程\n",
    "\n",
    "* 给定输入向量\n",
    "$$\\begin{equation} x_i = (x_{i1}, x_{i2}, \\cdots, x_{in})^T \\end{equation}$$\n",
    "* 计算隐藏层输出\n",
    "$$\\begin{equation} \\begin{split} \\textbf{h}_i &= (h_{i1}, h_{i2}, \\cdots, h_{im})^T \\\\ &= f_1(\\sum_{i=0}^nv_{1i}x_i, \\sum_{i=0}^nv_{2i}x_i, \\cdots, \\sum_{i=0}^nv_{mi}x_i) \\\\ &= f_1(\\textbf{v}\\textbf{x}) \\end{split} \\end{equation}$$\n",
    "* 计算预测输出\n",
    "$$\\begin{equation} \\begin{split} \\textbf{o}_i &= (o_{i1}, o_{i2}, \\cdots, o_{ik})^T \\\\ &= f_2(\\sum_{i=0}^mW_{1i}h_i, \\sum_{i=0}^mW_{2i}h_i, \\cdots, \\sum_{i=0}^mW_{ki}h_i) \\\\ &= f_2(\\textbf{W}\\textbf{h}) \\end{split} \\end{equation}$$\n",
    "* 计算损失\n",
    "    * 单个输入输出的损失\n",
    "$$\\begin{equation} \\mathcal{L}_i(\\textbf{V}, \\textbf{W}) = ||\\textbf{o}_i-\\textbf{y}_i||_2^2\\end{equation}$$\n",
    "    * 总损失\n",
    "$$\\begin{equation} \\mathcal{L}(\\textbf{V}, \\textbf{W}) = \\sum_{i=0}^t||\\textbf{o}_i-\\textbf{y}_i||_2^2\\end{equation}$$\n",
    "* 优化问题\n",
    "$$\\begin{equation} \\min\\limits_{\\textbf{V}, \\textbf{W}}\\mathcal{L}(\\textbf{V}, \\textbf{W}) = \\sum_{i=0}^t||\\textbf{o}_i-\\textbf{y}_i||_2^2 \\end{equation}$$\n",
    "\n",
    "### 误差反向传播过程（单输入）\n",
    "\n",
    "* 误差传递到输出层\n",
    "$$\\begin{equation}\\begin{split} \n",
    "\\frac{\\partial\\mathcal{L}(\\textbf{V}, \\textbf{W})}{\\partial{\\textbf{o}}} \n",
    "&= \\frac{\\partial ||\\textbf{o}-\\textbf{y}||_2^2}{\\partial{\\textbf{o}}} \\\\ \n",
    "&= \\frac{\\partial{\\sum_{j=1}^k(o_j-y_j)^2}}{\\partial{\\textbf{o}}}\\\\\n",
    "&= [\\frac{\\partial{\\sum_{j=1}^k(o_j-y_j)^2}}{\\partial{o_1}}, \\frac{\\partial{\\sum_{j=1}^k(o_j-y_j)^2}}{\\partial{o_2}}, \\cdots, \\frac{\\partial{\\sum_{j=1}^k(o_j-y_j)^2}}{\\partial{o_k}}]\\\\\n",
    "&= [2(o_1-y_1), 2(o_2-y_2), \\cdots, 2(o_k-y_k)] \\\\\n",
    "&= 2(\\textbf{o}-\\textbf{y})^T\\\\\n",
    "\\end{split}\\end{equation}$$\n",
    "* 误差传递到隐藏层\n",
    "    * 对于隐藏层到输出层的权重有\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split} \n",
    "    \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{W}} \n",
    "    &= \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{o}_i} \\frac{\\partial{\\textbf{o}_i}}{\\partial{\\textbf{W}\\textbf{h}}} \\frac{\\partial{\\textbf{W}\\textbf{h}}}{\\partial{\\textbf{W}}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial{\\textbf{o}}}{\\partial{\\textbf{W}\\textbf{h}}} = \\frac{\\partial{f_2(\\textbf{W}\\textbf{h})}}{\\partial{\\textbf{W}\\textbf{h}}} = diag(f_2^{\\prime}(\\textbf{W}\\textbf{h}))\n",
    "\\end{equation}\n",
    "$$\n",
    "由式-15、式-16以及式-17可得\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{W}\\textbf{h}} \n",
    "    &= 2(\\textbf{o}-\\textbf{y})^Tdiag(f_2^{\\prime}(\\textbf{W}\\textbf{h}))\\\\\n",
    "    &= \\boldsymbol{\\delta_1} \\\\\n",
    "    &= [\\delta_{11}, \\delta_{12}, \\cdots, \\delta_{1k}]\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "考虑到$\\textbf{W}\\textbf{h}$为向量，$\\textbf{W}$为矩阵，其直接微分会得到一个$k\\times k \\times m$的张量，这样不便于操作。考虑使用分量表示，即将$\\textbf{W}\\textbf{h}$表示为k个分量，分别操作。\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{W}}\n",
    "    &= \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{W}\\textbf{h}} \\frac{\\partial{\\textbf{W}\\textbf{h}}}{\\partial{\\textbf{W}}} \\\\\n",
    "    &= \\boldsymbol\\delta_1 \\frac{\\partial{\\textbf{W}\\textbf{h}}}{\\partial{\\textbf{W}}}\\\\\n",
    "    &= \\sum_{j=1}^k\\delta_{1j}\\frac{\\partial{\\sum_{i=1}^mW_{ji}h_i}}{\\partial{\\textbf{W}}}\\\\\n",
    "    &= \\boldsymbol{\\delta_1}^T\\textbf{h}^T \\\\\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "    * 对于隐藏层的输入有\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\mathcal{L}(\\textbf{V}, \\textbf{W})}}{\\partial{\\textbf{h}}}\n",
    "    &= \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{W}\\textbf{h}} \\frac{\\partial{\\textbf{W}\\textbf{h}}}{\\partial{\\textbf{h}}}\\\\\n",
    "    &= \\boldsymbol\\delta_1 \\frac{\\partial{\\textbf{W}\\textbf{h}}}{\\partial{\\textbf{h}}}\\\\\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\textbf{W}\\textbf{h}}}{\\partial{\\textbf{h}}}\n",
    "    &= \\frac{\\partial{((\\sum_{i=1}^mW_{1i}h_i, \\sum_{i=1}^mW_{2i}h_i, \\cdots, \\sum_{i=1}^mW_{ki}h_i)^T)}}{\\partial{((h_1, h_2, \\cdots, h_m)^T)}}\\\\\n",
    "    &= \n",
    "        \\left[\\begin{array}{cc}\n",
    "        W_{11}&W_{12}&\\cdots&W_{1m}\\\\\n",
    "        \\vdots&\\ddots&\\ddots&\\vdots\\\\\n",
    "        W_{k1}&W_{k2}&\\cdots&W_{km}\n",
    "        \\end{array}\\right]\\\\\n",
    "    &= \\textbf{W}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "由式-20和式-21可得\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\mathcal{L}(\\textbf{V}, \\textbf{W})}}{\\partial{\\textbf{h}}}\n",
    "    &= \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{W}\\textbf{h}} \\frac{\\partial{\\textbf{W}\\textbf{h}}}{\\partial{\\textbf{h}}}\\\\\n",
    "    &= \\boldsymbol\\delta_1 \\textbf{W}\\\\\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "* 误差传递到输入层\n",
    "    * 对于输入层到隐藏层的权重有\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\mathcal{L}(\\textbf{V}, \\textbf{W})}}{\\partial{\\textbf{V}}}\n",
    "    &= \\frac{\\partial{\\mathcal{L}(\\textbf{V}, \\textbf{W})}}{\\partial{\\textbf{h}}} \\frac{\\partial{\\textbf{h}}}{\\partial\\textbf{V}\\textbf{x}} \\frac{\\partial{\\textbf{V}\\textbf{x}}}{\\partial\\textbf{V}}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\textbf{h}}}{\\partial\\textbf{V}\\textbf{x}}\n",
    "    &= \\frac{\\partial{f_1(\\textbf{V}\\textbf{x})}}{\\partial{\\textbf{V}\\textbf{x}}} = diag(f_1^{\\prime}(\\textbf{V}\\textbf{x}))\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\mathcal{L}(\\textbf{V}, \\textbf{W})}}{\\partial{\\textbf{h}}} \\frac{\\partial{\\textbf{h}}}{\\partial{\\textbf{V}\\textbf{x}}}\n",
    "    &= \\boldsymbol{\\delta_1}\\textbf{W}diag(f_1^{\\prime}(\\textbf{V}\\textbf{x}))\\\\\n",
    "    &= \\boldsymbol{\\delta_2}\\\\\n",
    "    &= [\\delta_{21}, \\delta_{22}, \\cdots, \\delta_{2m}]\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "同上分析，考虑到$\\textbf{V}\\textbf{x}$为向量，$\\textbf{V}$为矩阵，其直接微分会得到一个$m\\times m \\times n$的张量，这样不便于操作。考虑使用分量表示，即将$\\textbf{V}\\textbf{x}$表示为k个分量，分别操作。\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{V}}\n",
    "    &= \\frac{\\partial{\\mathcal{L}(\\textbf{V},\\textbf{W})}}{\\partial\\textbf{V}\\textbf{x}} \\frac{\\partial{\\textbf{V}\\textbf{x}}}{\\partial{\\textbf{V}}} \\\\\n",
    "    &= \\boldsymbol\\delta_2 \\frac{\\partial{\\textbf{V}\\textbf{x}}}{\\partial{\\textbf{V}}}\\\\\n",
    "    &= \\sum_{j=1}^m\\delta_{2j}\\frac{\\partial{\\sum_{i=1}^nV_{ji}x_i}}{\\partial{\\textbf{V}}}\\\\\n",
    "    &= \\boldsymbol{\\delta_2}^T\\textbf{x}^T \\\\\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "* 综上可得\n",
    "待优化参数为$\\textbf{W}$和$\\textbf{V}$\n",
    "其中：\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial\\mathcal{L}(\\textbf{V}, \\textbf{W})}{\\partial{\\textbf{W}}}\n",
    "    &= \\boldsymbol{\\delta_1}^T\\textbf{h}^T\\\\\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial\\mathcal{L}(\\textbf{V}, \\textbf{W})}{\\partial{\\textbf{V}}}\n",
    "    &= \\boldsymbol{\\delta_2}^T\\textbf{x}^T\\\\\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\boldsymbol{\\delta_1} = 2(\\textbf{o}-\\textbf{y})^Tdiag(f_2^{\\prime}(\\textbf{W}\\textbf{h}))\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\boldsymbol{\\delta_2} = \\boldsymbol{\\delta_1}\\textbf{W}diag(f_1^{\\prime}(\\textbf{V}\\textbf{x}))\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "对于不同的激活函数有不同的导数值\n",
    "* Sigmoid\n",
    "$$\n",
    "\\begin{equation}\n",
    "S(x)=\\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "S^{\\prime}(x)=\\frac{e^{-x}}{(1+e^{-x})^2}=S(x)(1-S(x))\n",
    "\\end{equation}\n",
    "$$\n",
    "* ReLU\n",
    "$$\n",
    "\\begin{equation}\n",
    "R(x) = \\left\\{ \\begin{array}{cc} x,&x>0\\\\0,&x\\leq0 \\end{array} \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "R^{\\prime}(x) = \\left\\{ \\begin{array}{cc} 1,&x>0\\\\0,&x\\leq0 \\end{array} \\right.\n",
    "\\end{equation}\n",
    "$$\n",
    "* Tanh\n",
    "$$\n",
    "\\begin{equation}\n",
    "T(x) = \\tanh{x} = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "T^{\\prime}(x) = (\\tanh{x})^{\\prime} = 1 - \\tanh^2x\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "若激活函数$f_1$和$f_2$均为Sigmoid则有\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\boldsymbol{\\delta_1} = 2(\\textbf{o}-\\textbf{y})^Tdiag(S(\\textbf{W}\\textbf{h})\\cdot(1-S(\\textbf{W}\\textbf{h})))\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\boldsymbol{\\delta_2} = \\boldsymbol{\\delta_1}\\textbf{W}diag(S(\\textbf{V}\\textbf{x})\\cdot(1-S(\\textbf{V}\\textbf{x})))\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "基于符号运算的微分计算方法存在推导复杂的问题，尤其是对于层数较多的神经网络。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2ae398f54937c2e9a792e0245dde3e7f00c22003a4409849c6155f07537f255"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
