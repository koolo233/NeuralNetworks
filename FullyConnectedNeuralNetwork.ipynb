{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP网络/全连接神经网络\n",
    "\n",
    "BP网络为前向映射网络，网络由输入层、隐藏层以及输出层组成。\n",
    "\n",
    "## 基本单元\n",
    "\n",
    "BP网络的基本单元为含有激活函数的单个神经元\n",
    "\n",
    "$$o = f(x) = f(\\omega \\cdot \\textbf{x} + b) \\\\ \\omega = [\\omega_1, \\omega_2, \\cdots, \\omega_n]^T \\\\ x = [x_1, x_2, \\cdots, x_n]^T$$\n",
    "\n",
    "其中，$o$为输出，$\\omega$为权重向量， $\\textbf{x}$ 为输入向量，$b$ 为偏执, $f$为激活函数\n",
    "\n",
    "根据不同的任务类型，可以选用不同的激活函数\n",
    "\n",
    "## 基本架构\n",
    "\n",
    "通过多个基本单元组成一层，然后通过多层组成整个BP网络\n",
    "\n",
    "## BP网络的学习算法\n",
    "\n",
    "BP网络学习算法由两部分组成\n",
    "* 前向计算\n",
    "* 误差反向传播\n",
    "\n",
    "其中，前向传播过程是网络的应用过程；误差反向传播过程是BP网络权值学习和训练过程。\n",
    "\n",
    "**核心思想**：将输出误差以**某种形式**从输出层传播到隐藏层最后到输入层。因此，对于BP网络学习算法，最为重要的就是决定误差传播方式\n",
    "\n",
    "### 符号运算实现误差反向传播\n",
    "\n",
    "符号运算实现的误差反向传播方式要求首先对梯度进行符号运算并表示成已知量的运算，即**显式**的对每一个神经元的梯度运算进行推导。然后通过带入相应的量求得梯度，实现误差的传播和参数更新。\n",
    "\n",
    "### 自动微分实现误差反向传播\n",
    "\n",
    "自动微分实现的误差反向传播本质上是链式法则以及递归思想的综合。其不要求给出**显式**的每一个神经元的梯度计算符号表达式，而是根据链式法则以及递归，以深度优先搜索的方式对整个计算图中的待优化参数进行梯度计算。\n",
    "\n",
    "自动微分实现误差反向传播要求事先给出每一个运算的微分计算方式，即每一个运算不仅要给出其运算结果还需要给出其对应的微分计算方式。**对于不可导的位置，需要手动给出对应的微分结果**，例如在ReLU激活函数中，0位置不可导，因此需要手动赋予该处的导数为0.\n",
    "\n",
    "## 三层BP网络误差传递的符号推导\n",
    "\n",
    "### 定义\n",
    "定义一个三层BP网络，即输入层、一层隐藏层以及输出层\n",
    "\n",
    "参数定义如下：\n",
    "\n",
    "* 输入向量\n",
    "$$\\begin{equation} \\textbf{x} = (x_1, x_2, \\cdots, x_n)^T \\end{equation}$$\n",
    "* 输入层到隐藏层权重\n",
    "$$\\begin{equation} \\textbf{v}=\\left[\\begin{array}{cc} v_{11}&v_{12}& \\cdots&v_{1n} \\\\ \\vdots&\\vdots&\\ddots&\\vdots \\\\ v_{m1}&v_{m2}&\\cdots&v_{mn} \\end{array}\\right] \\end{equation}$$\n",
    "* 隐藏层激活函数\n",
    "$$\\begin{equation} f_1(\\textbf{v}\\textbf{x}) \\end{equation}$$\n",
    "* 隐藏层输出\n",
    "$$\\begin{equation} \\textbf{h} = (h_1, h_2, \\cdots, h_m)^T \\end{equation}$$\n",
    "* 隐藏层到输出权重\n",
    "$$\\begin{equation} \\textbf{w} = \\left[\\begin{array}{cc} w_{11}&w_{12}& \\cdots&w_{1m} \\\\ \\vdots&\\vdots&\\ddots&\\vdots \\\\ w_{k1}&w_{k2}&\\cdots&w_{km} \\end{array}\\right] \\end{equation}$$\n",
    "* 输出层激活函数\n",
    "$$\\begin{equation} f_2(\\textbf{\\textbf{w}\\textbf{h}}) \\end{equation}$$\n",
    "* 输出层输出\n",
    "$$\\begin{equation} \\textbf{o} = (o_1, o_2, \\cdots, o_k)^T \\end{equation}$$\n",
    "* 期望输出\n",
    "$$\\begin{equation} \\textbf{y} = (y_1, y_2, \\cdots, y_k)^T \\end{equation}$$\n",
    "\n",
    "### 正向运算过程\n",
    "\n",
    "* 给定输入向量\n",
    "$$\\begin{equation} x_i = (x_{i1}, x_{i2}, \\cdots, x_{in})^T \\end{equation}$$\n",
    "* 计算隐藏层输出\n",
    "$$\\begin{equation} \\begin{split} \\textbf{h}_i &= (h_{i1}, h_{i2}, \\cdots, h_{im})^T \\\\ &= f_1(\\sum_{i=0}^nv_{1i}x_i, \\sum_{i=0}^nv_{2i}x_i, \\cdots, \\sum_{i=0}^nv_{mi}x_i) \\\\ &= f_1(\\textbf{v}\\textbf{x}) \\end{split} \\end{equation}$$\n",
    "* 计算预测输出\n",
    "$$\\begin{equation} \\begin{split} \\textbf{o}_i &= (o_{i1}, o_{i2}, \\cdots, o_{ik})^T \\\\ &= f_2(\\sum_{i=0}^mw_{1i}h_i, \\sum_{i=0}^mw_{2i}h_i, \\cdots, \\sum_{i=0}^mw_{ki}h_i) \\\\ &= f_2(\\textbf{w}\\textbf{h}) \\end{split} \\end{equation}$$\n",
    "* 计算损失\n",
    "    * 单个输入输出的损失\n",
    "$$\\begin{equation} \\mathcal{L}_i(\\textbf{v}, \\textbf{w}) = (\\textbf{o}_i-\\textbf{y}_i)^2\\end{equation}$$\n",
    "    * 总损失\n",
    "$$\\begin{equation} \\mathcal{L}(\\textbf{v}, \\textbf{w}) = \\sum_{i=0}^t(\\textbf{o}_i-\\textbf{y}_i)^2\\end{equation}$$\n",
    "* 优化问题\n",
    "$$\\begin{equation} \\min\\limits_{\\textbf{v}, \\textbf{w}}\\mathcal{L}(\\textbf{v}, \\textbf{w}) = \\sum_{i=0}^t(\\textbf{o}_i-\\textbf{y}_i)^2 \\end{equation}$$\n",
    "\n",
    "### 误差反向传播过程"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2ae398f54937c2e9a792e0245dde3e7f00c22003a4409849c6155f07537f255"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('DIP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
