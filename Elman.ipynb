{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elman\n",
    "\n",
    "Elman网络大致可分为四层：输入层、隐藏层、承接层和输出层\n",
    "\n",
    "* 输入层：接受信号输入\n",
    "* 隐藏层：线性以及非线性变换\n",
    "* 输出层：加权输出\n",
    "* 承接层：承接上一时刻的信息\n",
    "\n",
    "相较于简单的BP网络，Elman网络具有承接层，从而对序列数据具有记忆能力，实现动态建模\n",
    "\n",
    "## 符号定义\n",
    "\n",
    "|符号|含义|\n",
    "|:-:|:-:|\n",
    "|$\\bm{x(t)}$|t时刻的输入信号|\n",
    "|$\\bm{h}$|隐藏层输出|\n",
    "|$\\bm{\\hat{y}}$|模型预测输出|\n",
    "|$\\bm{y}$|输入信号对应的真实输出|\n",
    "|$\\_i$|索引为i的值|\n",
    "|$^j\\_i$|第j个向量索引为i的值|\n",
    "|$\\bm{V}$|输入层到隐藏层的连接权矩阵|\n",
    "|$\\bm{b_1}$|输入层到隐藏层的偏置|\n",
    "|$\\bm{W}$|隐藏层到输出层的连接权矩阵|\n",
    "|$\\bm{b_2}$|隐藏层到输出层的偏置|\n",
    "|$f$|隐藏层激活函数|\n",
    "|$g$|输出层激活函数|\n",
    "|$n$|输入信号维度|\n",
    "|$k$|隐藏层维度|\n",
    "|$o$|输出层维度|\n",
    "|$t$|训练集总数|\n",
    "\n",
    "## 正向计算\n",
    "\n",
    "1. **输入层与承接层信号到隐藏层**\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\bm{h} = f(\\bm{V} [\\bm{x(t)}, \\bm{x(t-1)}] + \\bm{b_1})\n",
    "\\end{equation}\n",
    "$$\n",
    "2. **隐藏层到输出层**\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\bm{\\hat{y}} = g(\\bm{W} \\bm{h} + \\bm{b_2})\n",
    "\\end{equation}\n",
    "$$\n",
    "3. **损失计算**\n",
    "* 单个训练样本的损失\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L_j(\\bm{V}, \\bm{W}, \\bm{b_1}, \\bm{b_2})} = ||\\bm{\\hat{y}^j}-\\bm{y^j}||_2^2\n",
    "\\end{equation}\n",
    "$$\n",
    "* 训练集总损失\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L(\\bm{V}, \\bm{W}, \\bm{b_1}, \\bm{b_2})} = \\sum_{j=1}^t ||\\bm{\\hat{y}^j}-\\bm{y^j}||_2^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## 误差传递与参数更新\n",
    "\n",
    "上述正向推理和BP网络基本一致，唯一不同的是在输入层到隐藏层的时候需要同时考虑当前时刻的输入和上一时刻隐藏层的输出。\n",
    "\n",
    "这里值得注意的是：\n",
    "\n",
    "上述流程虽然看起来存在环，但是实际上是不存在的。Elman依然是**有向无环**的结构。一般Elman示意图会将上一时刻和当前时刻绘制在同一张图中，但是其结构可以展开为下图的结构。显然，对于当前时刻，整个模型并不存在环，因为误差仅在当前时刻的参数中传递，而不会传递到上一时刻，即不会通过**隐藏层-承接层-隐藏层**的路径进行误差传递。\n",
    "\n",
    "![Elman](https://raw.githubusercontent.com/koolo233/NeuralNetworks/main/images/Elman.png \"segment\")\n",
    "\n",
    "说回来，如果真的是有向有环图，Elman网络也不可能使用梯度下降法进行优化了\n",
    "\n",
    "经过上述分析，基本可以确定Elman网络和BP网络的梯度传递方式基本一致，唯一不同的是从输入层到隐藏层的权重$\\bm{V}\\notin\\mathcal{R}^{k\\times n}$而是$\\bm{V}\\in\\mathcal{R}^{k\\times (n+k)}$，其余部分和BP网络完全一致。\n",
    "\n",
    "完整的Elman网络误差传递与参数更新可以参考[BP网络误差传递与参数更新相关推导](https://github.com/koolo233/NeuralNetworks/blob/main/FullyConnectedNeuralNetwork.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c138c3a39b15f82bd4a9598693589d2dc2de979d592ed7357973539f0f36bd2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('flyai_pytorch1_5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
