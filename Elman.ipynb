{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elman\n",
    "\n",
    "Elman网络大致可分为四层：输入层、隐藏层、承接层和输出层\n",
    "\n",
    "* 输入层：接受信号输入\n",
    "* 隐藏层：线性以及非线性变换\n",
    "* 输出层：加权输出\n",
    "* 承接层：承接上一时刻的信息\n",
    "\n",
    "相较于简单的BP网络，Elman网络具有承接层，从而对序列数据具有记忆能力，实现动态建模\n",
    "\n",
    "## 符号定义\n",
    "\n",
    "|符号|含义|\n",
    "|:-:|:-:|\n",
    "|$\\bm{x(t)}$|t时刻的输入信号|\n",
    "|$\\bm{h}$|隐藏层输出|\n",
    "|$\\bm{\\hat{y}}$|模型预测输出|\n",
    "|$\\bm{y}$|输入信号对应的真实输出|\n",
    "|$\\_i$|索引为i的值|\n",
    "|$^j\\_i$|第j个向量索引为i的值|\n",
    "|$\\bm{V}$|输入层到隐藏层的连接权矩阵|\n",
    "|$\\bm{b_1}$|输入层到隐藏层的偏置|\n",
    "|$\\bm{W}$|隐藏层到输出层的连接权矩阵|\n",
    "|$\\bm{b_2}$|隐藏层到输出层的偏置|\n",
    "|$\\bm{U}$|承接层到隐藏层的连接权矩阵|\n",
    "|$f$|隐藏层激活函数|\n",
    "|$g$|输出层激活函数|\n",
    "|$n$|输入信号维度|\n",
    "|$k$|隐藏层维度|\n",
    "|$o$|输出层维度|\n",
    "|$d$|训练集总数|\n",
    "|$T$|总时间步|\n",
    "\n",
    "## 正向计算\n",
    "\n",
    "1. **输入层与承接层信号到隐藏层**\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\bm{h(t)} \n",
    "        &= f(\\bm{V}, \\bm{U}, \\bm{x(t)}, \\bm{h(t-1)}, \\bm{b_1}) \\\\\n",
    "        &= f(\\bm{V}\\bm{x(t)} + \\bm{U}\\bm{h(t-1)}] + \\bm{b_1})\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "2. **隐藏层到输出层**\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\bm{\\hat{y}} \n",
    "        &= g(\\bm{W}, \\bm{h(t)}, \\bm{b_2}) \\\\\n",
    "        &= g(\\bm{W} \\bm{h(t)} + \\bm{b_2})\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "3. **损失计算**\n",
    "* 单个训练样本的损失\n",
    "Elman要求t个时刻的损失均计入总损失，在这里使用$l_2$损失，当然，随着任务类型的变换可以换为交叉熵等\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L_j(\\bm{V}, \\bm{W}, \\bm{U}, \\bm{b_1}, \\bm{b_2})} = \\sum_{t=1}^T||\\bm{\\hat{y(t)}^j}-\\bm{y(t)^j}||_2^2\n",
    "\\end{equation}\n",
    "$$\n",
    "* 训练集总损失\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L(\\bm{V}, \\bm{W}, \\bm{U}, \\bm{b_1}, \\bm{b_2})} = \\sum_{j=1}^d\\sum_{t=1}^T ||\\bm{\\hat{y(t)}^j}-\\bm{y(t)^j}||_2^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## 误差传递与参数更新\n",
    "\n",
    "上述正向推理和BP网络基本一致，唯一不同的是在输入层到隐藏层的时候需要同时考虑当前时刻的输入和上一时刻隐藏层的输出。\n",
    "\n",
    "这里值得注意的是：\n",
    "\n",
    "上述流程虽然看起来存在环，但是实际上是不存在的。Elman依然是**有向无环**的结构。一般Elman示意图会将上一时刻和当前时刻绘制在同一张图中，但是其结构可以展开为下图的结构。显然，对于当前时刻，整个模型并不存在环，虽然误差会在不同时刻的参数中传递，但是并不会形成环。\n",
    "\n",
    "![Elman](https://raw.githubusercontent.com/koolo233/NeuralNetworks/main/images/Elman.png \"segment\")\n",
    "\n",
    "值得注意的是，Elman展开后虽然为一个有向无环图，但是其中的权重在不同时刻是**共享**的。\n",
    "\n",
    "说回来，如果真的是有向有环图，Elman网络也不可能使用梯度下降法进行优化了\n",
    "\n",
    "Elman网络的优化方法有两大类，分别为**实时循环学习(Real-Time Recurrent Learning, RTRL)**和**时序反向传播(BackPropagation Through Time, BPTT)**。这两类方法均基于BP算法，不同在于，RTRL在前向计算时就计算梯度并在每一个时间步更新参数，BPTT则在完成所有时间步的运算后进行梯度反向传播和参数更新。显然，BPTT需要保存大量的中间结果，其存储开销大；而RTRL则不需要存储多余的中间结果，其存储开销小，但是运算量大。\n",
    "\n",
    "在这里，主要时对BPTT进行推导\n",
    "\n",
    "### BPTT\n",
    "\n",
    "* 单时间步\n",
    "首先考虑仅有一个时间步，并将偏执并入权值，此时不存在沿时间的误差传递\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\frac{\\partial \\mathcal{L}}{\\partial \\bm{W}} \n",
    "        &= \\sum_{j=1}^d \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{W}} \\\\\n",
    "        &= \\sum_{j=1}^d \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{W}\\bm{h(1)^j}}\\frac{\\partial \\bm{W}\\bm{h(1)^j}}{\\partial \\bm{W}}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{W}\\bm{h(1)^j}} \n",
    "        &= 2(\\bm{\\hat{y(1)}^j}-\\bm{y(1)^j})^Tdiag(g^{\\prime}(\\bm{W}\\bm{h(1)^j})) \\\\\n",
    "        &= \\bm{\\delta_1^j(1)}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\bm{W}}\n",
    "    &= \\sum_{j=1}^d \\bm{\\delta_1^j(1)}^T\\bm{h(1)^j}^T\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\bm{h(1)^j}}\n",
    "    &= \\sum_{j=1}^d \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{W}\\bm{h(1)^j}}\\frac{\\partial \\bm{W}\\bm{h(1)^j}}{\\partial \\bm{h(1)^j}}\\\\ \n",
    "    &= \\sum_{j=1}^d \\bm{\\delta_1^j(1)}\\bm{W}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\frac{\\partial \\mathcal{L}}{\\partial \\bm{U}} \n",
    "        &= \\sum_{j=1}^d \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{U}} \\\\\n",
    "        &= \\sum_{j=1}^d \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{U}\\bm{h(0)^j}}\\frac{\\partial \\bm{U}\\bm{h(0)^j}}{\\partial \\bm{U}} \\\\\n",
    "        &= \\bm{0}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\frac{\\partial \\mathcal{L}}{\\partial \\bm{V}} \n",
    "        &= \\sum_{j=1}^d \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{V}} \\\\\n",
    "        &= \\sum_{j=1}^d \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{V}\\bm{x(1)^j}}\\frac{\\partial \\bm{V}\\bm{x(1)^j}}{\\partial \\bm{V}}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "        \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{V}\\bm{x(1)^j}} \n",
    "        &= \\frac{\\partial \\mathcal{L}}{\\partial \\bm{h(1)^j}}\\frac{\\partial \\bm{h(1)^j}}{\\partial \\bm{V}} \\\\\n",
    "        &= \\bm{\\delta_1^j(1)}\\bm{W}diag(f^{\\prime}(\\bm{V}\\bm{x(1)^j})) \\\\\n",
    "        &= \\bm{\\delta_2^j(1)}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\bm{V}} \n",
    "    &= \\sum_{j=1}^d \\frac{\\partial \\mathcal{L}_j}{\\partial \\bm{V}\\bm{x(1)^j}}\\frac{\\partial \\bm{V}\\bm{x(1)^j}}{\\partial \\bm{V}} \\\\\n",
    "    &= \\sum_{j=1}^d \\bm{\\delta_2^j(1)}^T\\bm{x(1)^j}^T\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "* 2时间步\n",
    "\n",
    "经过上述分析，基本可以确定Elman网络和BP网络的梯度传递方式基本一致，唯一不同的是从输入层到隐藏层的权重$\\bm{V}\\notin\\mathcal{R}^{k\\times n}$而是$\\bm{V}\\in\\mathcal{R}^{k\\times (n+k)}$，若将偏执也纳入权重，则可以得到$\\bm{V}\\in\\mathcal{R}^{k\\times (n+k+1)}$，其余部分和BP网络完全一致。\n",
    "\n",
    "完整的Elman网络误差传递与参数更新可以参考[BP网络误差传递与参数更新相关推导](https://github.com/koolo233/NeuralNetworks/blob/main/FullyConnectedNeuralNetwork.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "import os\n",
    "import random\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# seed\n",
    "random.seed(1024)\n",
    "np.random.seed(1024)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义单隐藏层Elman神经网络\n",
    "\n",
    "class MyElman(object):\n",
    "\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, lr) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # 模型权重\n",
    "        random.seed(1024)\n",
    "        np.random.seed(1024)\n",
    "        self.weight_input2hidden = np.random.randn(hidden_dims, input_dims+1+hidden_dims)\n",
    "        self.weight_hidden2output = np.random.randn(output_dims, hidden_dims)\n",
    "        \n",
    "        # 激活函数\n",
    "        self.activate = self.activate_func\n",
    "        # 激活函数微分，事先准备好\n",
    "        self.activate_dif = self.activate_dif_func\n",
    "\n",
    "        # 中间结果\n",
    "        # 输入数据\n",
    "        self.input_data = None\n",
    "        # 上一时刻数据\n",
    "        self.last_value = np.zeros((hidden_dims, 1))\n",
    "        # 真实的隐藏层输入\n",
    "        self.true_hidden_input = None\n",
    "        # 隐藏层输出，未激活\n",
    "        self.hidden_value = None\n",
    "        # 隐藏层输出，激活\n",
    "        self.hidden_activate_value = None\n",
    "\n",
    "        # 梯度结果\n",
    "        self.grad_hidden2output = None\n",
    "        self.grad_input2hidden = None\n",
    "\n",
    "        # 学习率\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"\n",
    "        正向计算\n",
    "        \"\"\"\n",
    "        \n",
    "        # 扩展一个维度\n",
    "        one_array = np.ones(len(input_data)).reshape(-1, 1)\n",
    "        self.input_data = np.concatenate((input_data, one_array), axis=1)\n",
    "        # input data : (batch, input_dims+1)\n",
    "\n",
    "        # 拼接输入数据与上一时刻数据\n",
    "        self.true_hidden_input = np.concatenate((self.input_data, self.last_value), axis=1)\n",
    "        # true hidden input : (batch, input_dims+hidden_dims+1)\n",
    "\n",
    "        # 计算隐藏层\n",
    "        self.hidden_value = np.matmul(self.weight_input2hidden, self.true_hidden_input.T)\n",
    "        self.hidden_activate_value = self.activate_func(self.hidden_value)\n",
    "        self.last_value = self.hidden_activate_value.copy()\n",
    "        # hidden_value: (hidden_dim, batch)\n",
    "\n",
    "        # 计算输出\n",
    "        output_value = np.matmul(self.weight_hidden2output, self.hidden_activate_value)\n",
    "        # output_value: (output_dim, batch)\n",
    "\n",
    "        return output_value.T\n",
    "    \n",
    "    def backward(self, label, pred):\n",
    "        \"\"\"\n",
    "        梯度反向传播\n",
    "        \"\"\"\n",
    "\n",
    "        # label: (batch, output_dim)\n",
    "        # pred: (batch, output_dim)\n",
    "\n",
    "        theta_1 = 2 * np.matmul(pred - label, np.eye(pred.shape[1]))\n",
    "        # theta_1: (batch, output_dim)\n",
    "        theta_2 = np.zeros((theta_1.shape[0], self.weight_hidden2output.shape[1]))\n",
    "        for i in range(theta_1.shape[0]):\n",
    "            theta_2[i, :] = np.matmul(np.matmul(theta_1[i:i+1, :], self.weight_hidden2output), \n",
    "                                      np.diag(self.activate_dif_func(self.hidden_activate_value[:, i]))).reshape(-1)\n",
    "\n",
    "        # theta_2: (batch, hidden_dim)\n",
    "\n",
    "        # 计算隐藏层到输出层权重的梯度\n",
    "        self.grad_hidden2output = np.matmul(theta_1.T, self.hidden_value.T)\n",
    "        # 计算输入层到隐藏层权重的梯度\n",
    "        self.grad_input2hidden = np.matmul(theta_2.T, self.true_hidden_input)\n",
    "    \n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        优化\n",
    "        \"\"\"\n",
    "        self.weight_input2hidden -= self.grad_input2hidden * self.lr\n",
    "        self.weight_hidden2output -= self.grad_hidden2output * self.lr\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def activate_func(x):\n",
    "        \"\"\"\n",
    "        sigmoid\n",
    "        \"\"\"\n",
    "        return 1/(1+np.e ** (-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def activate_dif_func(activate_result):\n",
    "        return activate_result*(1-activate_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试数据\n",
    "\n",
    "# 时间分量\n",
    "t_total = 10\n",
    "# 批大小\n",
    "batch_size = 5\n",
    "# 输入信号维度\n",
    "input_dims = 2\n",
    "# 输出信号维度\n",
    "output_dims = 1\n",
    "# 隐藏层维度\n",
    "hidden_num = 5\n",
    "# 学习率\n",
    "lr = 1e-3\n",
    "\n",
    "test_input_array = np.ones((t_total, batch_size, input_dims))\n",
    "test_output_array = np.random.randint(0, 2, (batch_size, output_dims))\n",
    "test_model = MyElman(input_dims=input_dims,\n",
    "                     hidden_dims=hidden_num, \n",
    "                     output_dims=output_dims, \n",
    "                     lr=lr)\n",
    "\n",
    "\n",
    "print(\"before optimize\")\n",
    "print(\"input to hidden weight\")\n",
    "print(test_model.weight_input2hidden)\n",
    "print(\"hidden to output weight\")\n",
    "print(test_model.weight_hidden2output)\n",
    "\n",
    "for t in t_total:\n",
    "    # 正向计算\n",
    "    test_pred = test_model.forward(test_input_array[t])\n",
    "    # 反向传播\n",
    "    test_model.backward(test_output_array, test_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c138c3a39b15f82bd4a9598693589d2dc2de979d592ed7357973539f0f36bd2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('flyai_pytorch1_5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
